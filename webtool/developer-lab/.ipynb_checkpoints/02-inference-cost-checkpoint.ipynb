{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developer Lab Notebook\n",
    "# Experiment: Measure Inference Cost and GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n7. The discussion section briefly mentions the model’s computational cost but lacks concrete details. \\nGiven that transformer-based models are known for their high resource requirements, it is important to \\nreport metrics such as average inference time per genome, GPU memory usage, and scalability. In addition, \\npotential solutions like model compression or inference-time optimization could be discussed to make the \\ntool more accessible for users with limited computational resources. (ok 5)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "7. The discussion section briefly mentions the model’s computational cost but lacks concrete details. \n",
    "Given that transformer-based models are known for their high resource requirements, it is important to \n",
    "report metrics such as average inference time per genome, GPU memory usage, and scalability. In addition, \n",
    "potential solutions like model compression or inference-time optimization could be discussed to make the \n",
    "tool more accessible for users with limited computational resources. (ok 5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CDS Prediction: 100%|██████████| 29/29 [04:14<00:00,  8.77s/it]\n",
      "CDS Prediction: 100%|██████████| 29/29 [04:24<00:00,  9.11s/it]\n",
      "TIS Prediction:  22%|██▏       | 41/189 [06:23<23:03,  9.35s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from api.core import AnnotatorPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "annotator = AnnotatorPipeline()\n",
    "genome_files = [\n",
    "    './data2-genome/bacteria-1.fasta',\n",
    "    './data2-genome/bacteria-2.fasta',\n",
    "    './data2-genome/bacteria-3.fasta'\n",
    "]\n",
    "results = []\n",
    "\n",
    "# Get GPU info\n",
    "def get_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(annotator.device)\n",
    "        return props.name, round(props.total_memory / (1024**3), 2)\n",
    "    return \"CPU\", 0\n",
    "\n",
    "gpu_name, gpu_total_mem = get_gpu_info()\n",
    "\n",
    "# Benchmark each genome\n",
    "for genome_file in genome_files:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    output_format = \"CSV\"\n",
    "    uuid = Path(genome_file).stem + \"-benchmark\"\n",
    "    tasks = {uuid: {\"progress\": 0, \"status\": \"\", \"result\": \"\", \"exec_state\": {}}}\n",
    "\n",
    "    start_gpu_mem = torch.cuda.memory_allocated() / (1024**2) if torch.cuda.is_available() else 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    result_path = annotator.pipeline(Path(genome_file), output_format, tasks, uuid, logging=None)\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_gpu_mem = torch.cuda.memory_allocated() / (1024**2) if torch.cuda.is_available() else 0\n",
    "\n",
    "    total_time = round(end_time - start_time, 2)\n",
    "    peak_mem = round(end_gpu_mem - start_gpu_mem, 2)\n",
    "    file_size_kb = os.path.getsize(genome_file) / 1024\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(result_path)\n",
    "        n_preds = len(df)\n",
    "        throughput = round(n_preds / total_time, 2) if total_time > 0 else 0\n",
    "    except:\n",
    "        n_preds = 0\n",
    "        throughput = 0\n",
    "\n",
    "    results.append({\n",
    "        \"Genome\": Path(genome_file).name,\n",
    "        \"GPU Name\": gpu_name,\n",
    "        \"GPU Total Memory (GB)\": gpu_total_mem,\n",
    "        \"Batch Size\": annotator.batch_size,\n",
    "        \"File Size (KB)\": round(file_size_kb, 2),\n",
    "        \"Inference Time (s)\": total_time,\n",
    "        \"Peak GPU Memory (MB)\": peak_mem,\n",
    "        \"Number of Predictions\": n_preds,\n",
    "        \"Throughput (seqs/sec)\": throughput,\n",
    "        \"Output Path\": str(result_path)\n",
    "    })\n",
    "\n",
    "# Save and visualize results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(f\"inference_performance_benchmarck-{gpu_name}.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "df_results.plot.bar(x=\"Genome\", y=\"Inference Time (s)\", title=\"Inference Time per Genome\", legend=False)\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_results.plot.bar(x=\"Genome\", y=\"Throughput (seqs/sec)\", title=\"Throughput per Genome\", color=\"orange\", legend=False)\n",
    "plt.ylabel(\"Sequences per Second\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved results to 'inference_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneLM",
   "language": "python",
   "name": "genelm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
