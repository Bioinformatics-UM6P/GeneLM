{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§ª Developer Lab Notebook\n",
    "# Experiment: Measure Inference Cost and GPU Usage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from core import AnnotatorPipeline\n",
    "\n",
    "# Load Model\n",
    "annotator = AnnotatorPipeline()\n",
    "\n",
    "# Upload Genome File\n",
    "# Provide a small FNA/FASTA genome file here\n",
    "genome_file = './webtool/developer-lab/sample_small_genome.fna'  # <-- Replace with your test file\n",
    "\n",
    "# Parameters\n",
    "output_format = \"CSV\"\n",
    "uuid = \"test-inference-cost\"\n",
    "tasks = {uuid: {\"progress\": 0, \"status\": \"\", \"result\": \"\", \"exec_state\": {}}}\n",
    "\n",
    "# Function to measure GPU memory\n",
    "def get_gpu_memory_mb():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Measure Inference Time\n",
    "# ----------------------------\n",
    "start_gpu_memory = get_gpu_memory_mb()\n",
    "\n",
    "start_time = time.time()\n",
    "result_path = annotator.pipeline(Path(genome_file), output_format, tasks, uuid, logging=None)\n",
    "end_time = time.time()\n",
    "\n",
    "end_gpu_memory = get_gpu_memory_mb()\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Results\n",
    "# ----------------------------\n",
    "total_time_sec = end_time - start_time\n",
    "max_gpu_used_mb = end_gpu_memory - start_gpu_memory\n",
    "\n",
    "print(f\"âœ… Inference completed!\")\n",
    "print(f\"Average Inference Time: {total_time_sec:.2f} seconds\")\n",
    "print(f\"Approximate GPU Memory Usage: {max_gpu_used_mb:.2f} MB\")\n",
    "print(f\"Output saved at: {result_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Summary Table\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\n",
    "    \"Metric\": [\"Inference Time (s)\", \"GPU Memory Usage (MB)\"],\n",
    "    \"Value\": [total_time_sec, max_gpu_used_mb]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "print(df_metrics)\n",
    "\n",
    "df_metrics.plot.bar(x=\"Metric\", y=\"Value\", legend=False)\n",
    "plt.title(\"Inference Cost Metrics\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 4. (Optional) Discussion Ideas\n",
    "# ----------------------------\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“š Discussion:\n",
    "- **Observed Time**: {:.2f} seconds per genome.\n",
    "- **Observed GPU Usage**: {:.2f} MB (rough).\n",
    "\n",
    "âœ… Possible optimizations:\n",
    "- Use **Mixed Precision Inference** (float16 instead of float32) with PyTorch `autocast`.\n",
    "- Use **ONNX export** + TensorRT for optimized inference.\n",
    "- Apply **model quantization** (8-bit) to reduce memory footprint.\n",
    "- **Batching larger inputs** to maximize GPU parallelism.\n",
    "\n",
    "âš¡ Potential future work: Add a lightweight distilled model for faster annotation on CPUs.\n",
    "\"\"\".format(total_time_sec, max_gpu_used_mb))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
